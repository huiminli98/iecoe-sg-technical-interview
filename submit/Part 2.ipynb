{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset\n",
    "\n",
    "Goal: Train 3 binary text-classfication models to detect Fake/True news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import gensim\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#nltk.download('wordnet')\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = pd.read_csv(\"True.csv\")\n",
    "false = pd.read_csv(\"Fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for the label we are predicting\n",
    "true['category'] = 1\n",
    "false['category'] = 0\n",
    "\n",
    "# Merge the two datasets\n",
    "df = pd.concat([true,false])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with empty elements\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop duplicate rows\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2211c2af1d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df.category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the dataset is balanced, no need to do re-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  category  \n",
       "0  December 31, 2017          1  \n",
       "1  December 29, 2017          1  \n",
       "2  December 31, 2017          1  \n",
       "3  December 30, 2017          1  \n",
       "4  December 29, 2017          1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge column \"title\" and \"text\"\n",
    "df['text'] = df['title'] + ' ' + df['text']\n",
    "df = df.drop(columns=['title', 'subject', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords, punctuations and single-character words from the text\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(punctuation)\n",
    "stop.update(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop and len(i)>1:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "df['text']=df['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing data\n",
    "x_train,x_test,y_train,y_test = train_test_split(df.text, df.category, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: GloVe + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe: an unsupervised learning algorithm for obtaining vector representations for words through training on aggregated global word-word co-occurrence statistics from a corpus.\n",
    "\n",
    "LSTM: a type of recurrent neural network capable of learning order dependence in sequence prediction problems.\n",
    "\n",
    "Code source: https://www.kaggle.com/madz2000/nlp-using-glove-embeddings-99-87-accuracy/notebook#Introduction-to-GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Text (Repsesent each word by a number)\n",
    "max_features = 10000\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "# Keep all news to exact 300 words\n",
    "maxlen = 300\n",
    "tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)\n",
    "\n",
    "tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding file\n",
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "EMBEDDING_FILE = 'glove.twitter.27B.100d.txt'\n",
    "# Map each word to its word vector\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, 'r', encoding='UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NTU\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "#change below line if computing normal stats is too slow\n",
    "embedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "embed_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network\n",
    "model = Sequential()\n",
    "# Non-trainable embeddidng layer\n",
    "model.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "# LSTM \n",
    "model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\n",
    "model.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\n",
    "model.add(Dense(units = 32 , activation = 'relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "131/131 [==============================] - 1489s 11s/step - loss: 0.3179 - accuracy: 0.8457 - val_loss: 0.0627 - val_accuracy: 0.9793\n",
      "Epoch 2/5\n",
      "131/131 [==============================] - 1602s 12s/step - loss: 0.0678 - accuracy: 0.9783 - val_loss: 0.0290 - val_accuracy: 0.9912\n",
      "Epoch 3/5\n",
      "131/131 [==============================] - 1635s 12s/step - loss: 0.0349 - accuracy: 0.9878 - val_loss: 0.0250 - val_accuracy: 0.9917\n",
      "Epoch 4/5\n",
      "131/131 [==============================] - 1680s 13s/step - loss: 0.0277 - accuracy: 0.9904 - val_loss: 0.0251 - val_accuracy: 0.9918\n",
      "Epoch 5/5\n",
      "131/131 [==============================] - 1727s 13s/step - loss: 0.0192 - accuracy: 0.9934 - val_loss: 0.0253 - val_accuracy: 0.9900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2430c5c46a0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NTU\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9899758346012709\n",
      "F1 score:  0.9899431456167861\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('F1 score: ', f1_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec: a technique to learn word embeddings from a large corpus of text using shallow neural network model.\n",
    "\n",
    "LSTM: a type of recurrent neural network capable of learning order dependence in sequence prediction problems.\n",
    "\n",
    "Code Source: https://www.kaggle.com/atishadhikari/fake-news-cleaning-word2vec-lstm-99-accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['category'].values\n",
    "# Convert X to format acceptable by gensim, remove stopwords and punctuations in the process\n",
    "X = []\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "for par in df[\"text\"].values:\n",
    "    tmp = []\n",
    "    sentences = nltk.sent_tokenize(par)\n",
    "    for sent in sentences:\n",
    "        sent = sent.lower()\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        filtered_words = [w.strip() for w in tokens if w not in stop and len(w) > 1]\n",
    "        tmp.extend(filtered_words)\n",
    "    X.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Create word vectors using Word2Vec method \n",
    "w2v_model = gensim.models.Word2Vec(sentences=X, vector_size=EMBEDDING_DIM, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text: Repsesent each word by a number\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep all news length to 700\n",
    "maxlen = 700 \n",
    "X = pad_sequences(X, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer creates one more vector for \"UNKNOWN\" words, or padded words (0s). This Vector is filled with zeros.\n",
    "# Thus our vocab size inceeases by 1\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Create weight matrix from word2vec model\n",
    "def get_weight_matrix(model, vocab):\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = model.wv[word]\n",
    "    return weight_matrix\n",
    "\n",
    "embedding_vectors = get_weight_matrix(w2v_model, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network\n",
    "model = Sequential()\n",
    "# Non-trainable embeddidng layer\n",
    "model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))\n",
    "# LSTM \n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "734/734 [==============================] - 733s 994ms/step - loss: 0.1719 - acc: 0.9371 - val_loss: 0.0404 - val_acc: 0.9893\n",
      "Epoch 2/5\n",
      "734/734 [==============================] - 654s 892ms/step - loss: 0.0500 - acc: 0.9838 - val_loss: 0.0159 - val_acc: 0.9951\n",
      "Epoch 3/5\n",
      "734/734 [==============================] - 562s 765ms/step - loss: 0.0128 - acc: 0.9970 - val_loss: 0.0111 - val_acc: 0.9971\n",
      "Epoch 4/5\n",
      "734/734 [==============================] - 550s 750ms/step - loss: 0.0070 - acc: 0.9983 - val_loss: 0.0181 - val_acc: 0.9962\n",
      "Epoch 5/5\n",
      "734/734 [==============================] - 502s 683ms/step - loss: 0.0070 - acc: 0.9978 - val_loss: 0.0256 - val_acc: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28280b604a8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_split=0.3, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9958829320683791\n",
      "F1 score:  0.9958718461057823\n"
     ]
    }
   ],
   "source": [
    "y_pred = (model.predict(X_test) >= 0.5).astype(\"int\")\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('F1 score: ', f1_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: TF-iDF + RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-iDF: a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "RandomForest: a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "Code Source: https://www.kaggle.com/rahulvv/nb-and-rf-models-99-accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = df['text'].values\n",
    "y_data = df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and process text data\n",
    "def final(X_data_full):\n",
    "    \n",
    "    # function for removing punctuations\n",
    "    def remove_punct(X_data_func):\n",
    "        string1 = X_data_func.lower()\n",
    "        translation_table = dict.fromkeys(map(ord, punctuation),' ')\n",
    "        string2 = string1.translate(translation_table)\n",
    "        return string2\n",
    "    \n",
    "    X_data_full_clear_punct = []\n",
    "    for i in range(len(X_data_full)):\n",
    "        test_data = remove_punct(X_data_full[i])\n",
    "        X_data_full_clear_punct.append(test_data)\n",
    "        \n",
    "    # function to remove stopwords\n",
    "    def remove_stopwords(X_data_func):\n",
    "        pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "        string2 = pattern.sub(' ', X_data_func)\n",
    "        return string2\n",
    "    \n",
    "    X_data_full_clear_stopwords = []\n",
    "    for i in range(len(X_data_full)):\n",
    "        test_data = remove_stopwords(X_data_full[i])\n",
    "        X_data_full_clear_stopwords.append(test_data)\n",
    "        \n",
    "    # function for tokenizing\n",
    "    def tokenize_words(X_data_func):\n",
    "        words = nltk.word_tokenize(X_data_func)\n",
    "        return words\n",
    "    \n",
    "    X_data_full_tokenized_words = []\n",
    "    for i in range(len(X_data_full)):\n",
    "        test_data = tokenize_words(X_data_full[i])\n",
    "        X_data_full_tokenized_words.append(test_data)\n",
    "        \n",
    "    # function for lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def lemmatize_words(X_data_func):\n",
    "        words = lemmatizer.lemmatize(X_data_func)\n",
    "        return words\n",
    "    \n",
    "    X_data_full_lemmatized_words = []\n",
    "    for i in range(len(X_data_full)):\n",
    "        test_data = lemmatize_words(X_data_full[i])\n",
    "        X_data_full_lemmatized_words.append(test_data)\n",
    "        \n",
    "    # creating the bag of words model\n",
    "    cv = CountVectorizer(max_features=1000)\n",
    "    X_data_full_vector = cv.fit_transform(X_data_full_lemmatized_words).toarray()\n",
    "    \n",
    "    \n",
    "    tfidf = TfidfTransformer()\n",
    "    X_data_full_tfidf = tfidf.fit_transform(X_data_full_vector).toarray()\n",
    "    \n",
    "    return X_data_full_tfidf\n",
    "\n",
    "data_X = final(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Random Forest model\n",
    "rfc = RandomForestClassifier(n_estimators= 10)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9951669202541842\n",
      "F1 score:  0.9951604985566729\n"
     ]
    }
   ],
   "source": [
    "y_pred = rfc.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('F1 score: ', f1_score(y_test, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Emedding Model | Classification Model | Accuracy | F1 Score |\n",
    "| -: | -: | -: | -: | -: |\n",
    "| Model 1: GloVe + LSTM | GloVe | LSTM | 0.9899758346012709 | 0.9899431456167861 |\n",
    "| Model 2: Word2Vec + LSTM | Word2Vec | LSTM | __0.9958829320683791__ | __0.9958718461057823__ |\n",
    "| Model 3: TF-iDF + RandomForest | TF-iDF | Random Forest | 0.9951669202541842 | 0.9951604985566729 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most preferred: Model 2: Word2Vec + LSTM\n",
    "\n",
    "Reason for choosing each model:\n",
    "> Model 1: GloVe + LSTM\n",
    "- GloVe: This technique derives semantic relationships between words from the co-occurrence matrix. It does not rely just on local context information of words (compared with Word2Vec), so that it incorporates global statistics to obtain word vectors.\n",
    "- LSTM: LSTM introduces more controlling knobs (than traditional RNN), which control the flow and mixing of inputs. Thus, LSTM can understand context along with recent dependency to help solve this classification problem. Also, it is good at dealing with the vanishing gradient problem while training.\n",
    "\n",
    "> Model 2: Word2Vec + LSTM\n",
    "- Word2Vec: This is one of the most popular technique to learn word embeddings. It captures context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
    "\n",
    "> Model 3: TF-iDF + RandomForest\n",
    "- TF-iDF: TF-iDF computes word frequency scores that highlight words that are most interested. In this case, it can be a good method to encode news.\n",
    "- Random Forest: RF consists of multiple single decision trees (each based on a random sample of the training data) and uses averaging to improve accuracy and control over-fitting. It is generally more accurate and stable than single trees. Therefore, it is a good choice for classification problems among large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
